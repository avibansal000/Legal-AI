{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # import required module\n",
    "import os\n",
    "import xml.etree.cElementTree as ET;\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "import nltk as nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# assign directory\n",
    "def remove_stop_words(sen):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(sen)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    sen=filtered_sentence\n",
    "    return sen\n",
    "def remove_stop_words_list(sen):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in sen:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    sen=filtered_sentence\n",
    "    return sen\n",
    "\n",
    "\n",
    "directory = 'CA Cases_with HN alongwith Bold_Italic'\n",
    "f=open(\"facts_cue_phrases.txt\",'r')\n",
    "facts=f.readlines()\n",
    "for i in range(len(facts)):\n",
    "    facts[i]=facts[i].strip()\n",
    "    facts[i]=facts[i].lower()\n",
    "    # facts[i]=remove_stop_words(facts[i])\n",
    "f=open(\"arguments_cue_phrases.txt\",'r')\n",
    "arguments=f.readlines()\n",
    "for i in range(len(arguments)):\n",
    "    arguments[i]=arguments[i].strip()\n",
    "    arguments[i]=arguments[i].lower()\n",
    "    # arguments[i]=remove_stop_words(arguments[i])\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "\n",
    "for filename in os.listdir(directory):  \n",
    "    f = os.path.join(directory, filename)\n",
    "    # f=\"Civil Appeal/12 01 2021 SC 5231-32 of 2016.xml\"\n",
    "    print(f)\n",
    "    \n",
    "    f1=open(f,'r')\n",
    "    # print(f1)\n",
    "    tree=ET.ElementTree(file=f)\n",
    "    root=tree.getroot()\n",
    "    # print(f)\n",
    "    allpara=[]\n",
    "    for chld in root:\n",
    "        # print(chld)\n",
    "        if(chld.tag=='JudgmentText'):\n",
    "            # print(chld)\n",
    "            for para in chld:\n",
    "                if(para.tag!='I'):\n",
    "                    txt=para.text\n",
    "                    if(txt!=None and len(txt)>0):\n",
    "                        txt=txt.strip()\n",
    "                        txt=txt.lower()\n",
    "                        allpara.append(txt)\n",
    "                # else:\n",
    "                #     allpara.append(\"STOP\")\n",
    "    # print(allpara)\n",
    "    fact_on=0\n",
    "    data=[]\n",
    "    for i in allpara:\n",
    "        ok=False\n",
    "        # i=remove_stop_words(i)\n",
    "        if(fact_on==1 or fact_on==0):\n",
    "            if(i=='STOP'):\n",
    "                fact_on=0\n",
    "            for j in arguments:\n",
    "                if(len(i)<2*len(j)):\n",
    "                    continue\n",
    "                if(fuzz.partial_ratio(j,i)>80):\n",
    "                    # print(\"argument\")\n",
    "                    # print(len(i))\n",
    "                    # print(i)\n",
    "                    # print(len(j))\n",
    "                    # print(\"-----------------Fact end-------------------------\")\n",
    "                    fact_on=-1\n",
    "                    break\n",
    "        if(fact_on==0):\n",
    "            for j in facts:\n",
    "                if(fuzz.partial_ratio(j,i)>90):\n",
    "                    # print(i)\n",
    "                    # print(\"----------Fact start-----------\")\n",
    "                    # print(j)\n",
    "                    # print(i)\n",
    "                    fact_on=1\n",
    "                    # ok=True\n",
    "                    \n",
    "        if( not ok):\n",
    "            # print(i)\n",
    "            text = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(text)\n",
    "            tense = {}\n",
    "            tense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n",
    "            tense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n",
    "            tense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]]) \n",
    "            # print(tense)\n",
    "            if(fact_on==1):\n",
    "                if(tense[\"present\"]>tense[\"future\"] and tense[\"present\"]>tense[\"past\"]):\n",
    "                    fact_on=0\n",
    "            elif(fact_on==0):\n",
    "                if(tense[\"past\"]>=tense[\"future\"] and tense[\"present\"]<=tense[\"past\"]):\n",
    "                    fact_on=1\n",
    "                \n",
    "        \n",
    "        if(fact_on==1):\n",
    "            data.append(i)\n",
    "    f=os.path.join(\"result\", os.path.splitext(filename)[0]+\".txt\")\n",
    "    # f=\"result/12 01 2021 SC 5231-32 of 2016.txt\"\n",
    "    f1=open(f,\"w\")\n",
    "    l=\"\"\n",
    "    for i in data:\n",
    "        i=i+'\\n'\n",
    "        l=l+i\n",
    "    # print(l)\n",
    "    # print(f1)\n",
    "    f1.write(l)\n",
    "    # print(l)\n",
    "    # print(f1)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA Cases_with HN alongwith Bold_Italic\n"
     ]
    }
   ],
   "source": [
    "# Code for correcting xml files \n",
    "directory = 'CA Cases_with HN alongwith Bold_Italic'\n",
    "print(directory)\n",
    "import os\n",
    "# assign directory\n",
    "i=0\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory,filename)\n",
    "    f1=open(f,'r')\n",
    "    # print(f)\n",
    "    data=f1.read()\n",
    "    # data=data.replace(\"<JudgmentText>\",\"<JudgmentText><P>\")\n",
    "    # data=data.replace(\"</JudgmentText>\",\"</P></JudgmentText>\")\n",
    "    # data=data.replace(\"<P/>\",\"</P><P>\")\n",
    "    # data=data.replace(\"<P />\",\"</P><P>\")\n",
    "    data=data.replace(\"</EditorsNote>\",\"</P></EditorsNote>\")\n",
    "    data=data.replace(\"<EditorsNote>\",\"<EditorsNote><P>\")\n",
    "    # data=data.replace(\"<B>\",\"</P><B><P>\")\n",
    "    # data=data.replace(\"</P></P><ILOLeg>\",\"</P><ILOLeg>\")\n",
    "    # data=data.replace(\"<ILOLeg\",\"</P><ILOLeg\")\n",
    "    # data=data.replace(\"<Table>\",\"</P><Table><P>\")\n",
    "    # data=data.replace(\"</Table>\",\"</P></Table><P>\")\n",
    "    f1.close()\n",
    "    # print(data)\n",
    "    # if(i==0):\n",
    "    f1=open(f,\"w\")\n",
    "    f1.write(data)\n",
    "    # i+=1\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99cd7ad894614d92c50672e4a02f8b10aeb6364001823b3c506298aab29ab573"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
